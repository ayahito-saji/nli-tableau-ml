{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5f9f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import pickle\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdb295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./config.json\", \"r\") as fp:\n",
    "    config = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3811d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR_PATH = config[\"data_dir_path\"]\n",
    "LABEL_DICT = config[\"label_dict\"]\n",
    "WORD_EMBEDDINGS_PATH = os.path.join(DATA_DIR_PATH, \"glove.840B.300d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0801d2cf",
   "metadata": {},
   "source": [
    "# Generate Word Index Map and Word Embedding Matrix from UD Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1d4ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snli_train = pd.read_csv(os.path.join(DATA_DIR_PATH, \"snli_train.tsv\"), delimiter='\\t', index_col=0)\n",
    "df_snli_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9345963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "for sample in tqdm(list(df_snli_train.itertuples())):\n",
    "    word_list.extend(tree2tokenlist(ET.fromstring(sample.udtree1)))\n",
    "    word_list.extend(tree2tokenlist(ET.fromstring(sample.udtree2)))\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aedc29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts = Counter(word_list)\n",
    "\n",
    "word2index = {}\n",
    "\n",
    "word2index[\"_PAD_\"] = 0\n",
    "word2index[\"_OOV_\"] = 1\n",
    "\n",
    "offset = 2\n",
    "\n",
    "for i, word in enumerate(counts.most_common()):\n",
    "    word2index[word[0]] = i + offset\n",
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626a8e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings = {}\n",
    "\n",
    "with open(WORD_EMBEDDINGS_PATH, \"r\") as fp:\n",
    "    for line in fp:\n",
    "        line = line.split()\n",
    "\n",
    "        try:\n",
    "            float(line[1])\n",
    "            word = line[0]\n",
    "            if word in word2index:\n",
    "                embeddings[word] = np.array(line[1:], dtype=float)\n",
    "\n",
    "        except ValueError:\n",
    "            pass\n",
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a866b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2index) # 埋め込みの単語数\n",
    "embedding_size = len(list(embeddings.values())[0]) # 単語埋め込みの次元数\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_size)) # 埋め込み行列(単語数×埋め込み次元数)\n",
    "\n",
    "missing_words = []\n",
    "for word, index in word2index.items():\n",
    "    if word in embeddings:\n",
    "        embedding_matrix[index] = embeddings[word]\n",
    "    else:\n",
    "        if word == \"_PAD_\":\n",
    "            continue\n",
    "        if word != \"_OOV_\":\n",
    "            missing_words.append(word)\n",
    "        embedding_matrix[index] = np.random.normal(size=(embedding_size))\n",
    "\n",
    "print(\"The number of missing words:\", len(missing_words))\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c62f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_DIR_PATH, \"word_index_map.json\"), \"w\") as fp:\n",
    "    json.dump(word2index, fp, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a81527",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_DIR_PATH, \"word_embedding_matrix.pkl\"), \"wb\") as fp:\n",
    "    pickle.dump(embedding_matrix, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcdf999",
   "metadata": {},
   "source": [
    "# Generate Train Dataset, Dev Dataset, Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fe2735",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_snli_dev = pd.read_csv(os.path.join(DATA_DIR_PATH, \"snli_dev.tsv\"), delimiter='\\t', index_col=0)\n",
    "df_snli_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09700025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(df):\n",
    "    id_list = []\n",
    "    premise_list = []\n",
    "    hypothesis_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    for sample in tqdm(list(df.itertuples())):\n",
    "        id_list.append(sample.Index)\n",
    "        premise_list.append(tokenlist2indexlist(tree2tokenlist(ET.fromstring(sample.udtree1)), word2index))\n",
    "        hypothesis_list.append(tokenlist2indexlist(tree2tokenlist(ET.fromstring(sample.udtree2)), word2index))\n",
    "        label_list.append(LABEL_DICT[sample.gold_label])\n",
    "        \n",
    "    return {\n",
    "        \"ids\": id_list,\n",
    "        \"premises\": premise_list,\n",
    "        \"hypotheses\": hypothesis_list,\n",
    "        \"labels\": label_list\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbe6415",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df_to_dataset(df_snli_train)\n",
    "with open(os.path.join(DATA_DIR_PATH, \"train_data.pkl\"), \"wb\") as fp:\n",
    "    pickle.dump(train_data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33b55cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = df_to_dataset(df_snli_dev)\n",
    "with open(os.path.join(DATA_DIR_PATH, \"dev_data.pkl\"), \"wb\") as fp:\n",
    "    pickle.dump(dev_data, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
